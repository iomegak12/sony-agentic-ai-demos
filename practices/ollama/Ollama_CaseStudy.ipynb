{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkm_L4rL0qQ6",
        "outputId": "9673c255-e9af-44a5-d559-9758923fdd48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/981.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers langdetect deep-translator  langchain langchain-community langchain-ollama gradio -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "U5GS7hE203dN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"mistral\"\n",
        "temperature = 0.1\n",
        "max_tokens = 2000\n",
        "llm = ChatOllama(\n",
        "    model = model_name,\n",
        "    temperature=temperature,\n",
        "    max_tokens=max_tokens,\n",
        "    base_url = \"http://164.52.203.42:11434/\"\n",
        ")"
      ],
      "metadata": {
        "id": "a534fgCq09jO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "You're a native internal AI assistant. Help users with their important tasks, like a professor in a particular field.\n",
        "\n",
        "Query: {query}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables = [\"query\"],\n",
        "    template = template\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "chain = prompt | llm\n",
        ""
      ],
      "metadata": {
        "id": "Pk_W4V1f1E4B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "class Translation:\n",
        "  def __init__(self, text, destination):\n",
        "    self.text = text\n",
        "    self.destination = destination\n",
        "\n",
        "    try:\n",
        "      self.original = detect(self.text)\n",
        "    except Exception as error:\n",
        "      print(f\"Error Occurred, Details : {error}\")\n",
        "      self.original = \"auto\"\n",
        "\n",
        "  def translate(self):\n",
        "    translator = GoogleTranslator(source=self.original, target=self.destination)\n",
        "    translation = translator.translate(self.text)\n",
        "\n",
        "    return translation"
      ],
      "metadata": {
        "id": "i0zwKhJC1IdP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "v85ONFys1Lba"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reply(message, history):\n",
        "  txt = Translation(message, \"en\")\n",
        "\n",
        "  if txt.original == \"en\":\n",
        "    response = chain.invoke({\n",
        "        \"query\": message\n",
        "    })\n",
        "\n",
        "    return response\n",
        "  else:\n",
        "    translation = txt.translate()\n",
        "\n",
        "    response = chain.invoke({\n",
        "        \"query\": translation\n",
        "    })\n",
        "\n",
        "    t = Translation(response.content, txt.original)\n",
        "    final_response = t.translate()\n",
        "\n",
        "    return final_response"
      ],
      "metadata": {
        "id": "_-RF5YY_1Osq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is Deep Learning?\"\n",
        "\n",
        "result = reply(question, history = [])"
      ],
      "metadata": {
        "id": "pfWScsV01RqN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "VzmL9Z_D1Ucs",
        "outputId": "d0685ff9-68c3-4b78-8df3-b345198bd5b1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Deep Learning is a subset of Machine Learning (ML) that is based on artificial neural networks with representation learning. It attempts to model high-level abstractions in data by using architecture inspired by the structure and function of the brain called artificial neural networks. These networks automatically learn to represent data for solving complex problems, such as image recognition, natural language processing, and speech recognition, without being explicitly programmed.\\n\\nIn deep learning, multiple layers are used to learn increasingly abstract representations of the input data. The layers in a deep neural network are connected in a cascading manner, allowing information to flow from one layer to the next. This process is known as forward propagation. During training, the model adjusts its internal parameters (weights and biases) to minimize the error between the predicted output and the actual output.\\n\\nDeep learning has revolutionized many fields by enabling computers to learn from large amounts of data and make accurate predictions or decisions without being explicitly programmed for each specific task. Examples include self-driving cars, speech recognition systems like Siri and Alexa, image recognition in social media platforms, and recommendation systems on e-commerce websites.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"ரியல் எஸ்டேட்டில் அதிக பணம் சம்பாதிப்பது எப்படி?\"\n",
        "\n",
        "result = reply(question, history = [])"
      ],
      "metadata": {
        "id": "UgH2mYZy1W3j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "8szFAjg71fmG",
        "outputId": "fdb94b82-2969-4e90-f09e-654607c6615d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ரியல் எஸ்டேட்டிலிருந்து உங்கள் வருமானத்தை அதிகரிக்க, பின்வரும் உத்திகளைக் கவனியுங்கள்:\\n\\n1. குறைந்த வாங்கவும், அதிக விற்கவும்: ரியல் எஸ்டேட் உட்பட எந்தவொரு முதலீட்டிலும் பணம் சம்பாதிப்பதற்கான அடிப்படைக் கொள்கை இது. குறைத்து மதிப்பிடப்பட்ட மற்றும் பாராட்டுக்கான சாத்தியக்கூறுகளைக் கொண்ட பண்புகளைத் தேடுங்கள்.\\n\\n2. வாடகை வருமானம்: நிலையான வருமான ஸ்ட்ரீமை உருவாக்க வாடகை சொத்துக்களில் முதலீடு செய்யுங்கள். இலாபகரமான வாடகை வாய்ப்புகளைக் கண்டறிய உள்ளூர் சந்தையை ஆராய்ச்சி செய்யுங்கள்.\\n\\n3. ஹவுஸ் புரட்டுதல்: ஒரு சொத்தை குறைந்த விலையில் வாங்கவும், புதுப்பிக்கவும், லாபத்திற்காக விற்கவும். இதற்கு வீட்டு மேம்பாடு குறித்த அறிவு மற்றும் செலவுகளை துல்லியமாக மதிப்பிடுவதற்கான திறன் தேவை.\\n\\n4. ரியல் எஸ்டேட் முதலீட்டு அறக்கட்டளைகள் (REIT கள்): இவை வருமானத்தை ஈட்டக்கூடிய ரியல் எஸ்டேட் சொந்தமாக அல்லது நிதியளிக்கும் நிறுவனங்கள். சொத்துக்களை நேரடியாக வாங்காமல் பெரிய அளவிலான ரியல் எஸ்டேட்டில் முதலீடு செய்ய அவை உங்களை அனுமதிக்கின்றன.\\n\\n5. வணிக ரியல் எஸ்டேட்: அலுவலக கட்டிடங்கள், சில்லறை இடங்கள் அல்லது தொழில்துறை வசதிகள் போன்ற வணிக சொத்துக்களில் முதலீடு செய்வதைக் கவனியுங்கள். அதிக வாடகை மற்றும் நீண்ட குத்தகைகளுக்கான சாத்தியக்கூறுகள் காரணமாக குடியிருப்பு பண்புகளை விட இவை அதிக வருமானத்தை வழங்க முடியும்.\\n\\n6. ரியல் எஸ்டேட் கூட்டாண்மை: வளங்களை பூல் செய்ய மற்ற முதலீட்டாளர்களுடன் கூட்டாளர் மற்றும் பெரிய, அதிக லாபகரமான சொத்துக்களில் முதலீடு செய்யுங்கள். இது ஆபத்தை பரப்பவும் உதவும்.\\n\\n7. சொத்து மேலாண்மை: நீங்கள் வாடகை சொத்துக்களை வைத்திருந்தால், நிர்வாகக் கட்டணத்தில் சேமிக்க அவற்றை நிர்வகிப்பதைக் கவனியுங்கள். இருப்பினும், இதற்கு சொத்து மேலாண்மை நடைமுறைகளின் நேரமும் அறிவும் தேவை.\\n\\n8. கல்வி மற்றும் நெட்வொர்க்கிங்: ரியல் எஸ்டேட் முதலீடு பற்றிய கல்வியில் முதலீடு செய்யுங்கள் நிபுணர்களிடமிருந்து கற்றுக்கொள்ளவும் சந்தை போக்குகளைப் பற்றி புதுப்பிக்கவும். அறிவு மற்றும் வாய்ப்புகளைப் பகிர்ந்து கொள்ள மற்ற முதலீட்டாளர்களுடன் நெட்வொர்க்.\\n\\n9. பன்முகப்படுத்தவும்: உங்கள் முட்டைகள் அனைத்தையும் ஒரே கூடையில் வைக்க வேண்டாம். ஆபத்தை குறைப்பதற்கும் சாத்தியமான வருமானத்தை அதிகரிப்பதற்கும் பல்வேறு வகையான பண்புகள், இருப்பிடங்கள் மற்றும் உத்திகள் முழுவதும் உங்கள் முதலீடுகளை பரப்பவும்.\\n\\n10. நீண்ட கால உத்தி: ரியல் எஸ்டேட் பெரும்பாலும் நீண்ட கால முதலீடாகும். பொறுமையாகவும், வருமானத்தை அதிகரிக்க பல ஆண்டுகளாக சொத்துக்களைப் பிடிக்கவும் தயாராக இருங்கள்.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.ChatInterface(fn=reply, title = \"Multi-Lingual ChatBot\")\n",
        "\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "Ji9BY5xc1l3y",
        "outputId": "d9e77a56-c19a-46a4-86cd-b23cb28dd10f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:334: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://63af72c977efe32a82.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://63af72c977efe32a82.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://63af72c977efe32a82.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}